from urllib.request import urlopen
from urllib.parse import urlencode
from bs4 import BeautifulSoup
import re
import time
import json
import requests
import urllib
cookie = 'miid=943244687302250720; cna=GU/cEzHdw0oCAXALgH6Lbklq; enc=OOHf75WDH9e%2Fyb3jIcgcHGpQYeP4g5pRn2dcQriko1e9i' \
         '%2BEGfuOei5jNgDriTXf6Dx0HffjoAfz361L0s7VWGQ%3D%3D; thw=cn; hng=CN%7Czh-CN%7CCNY%7C156; t=29be219508acc3fe8' \
         '0dd5b482ab86030; uc3=vt3=F8dByR1QKxIgLrdo5t8%3D&id2=UUGq2LxDmeBy4g%3D%3D&nk2=DeJbtiq%2F4vfV&lg2=UIHiLt3xD8' \
         'xYTw%3D%3D; tracknick=nekoman%5Cu4E36; lgc=nekoman%5Cu4E36; _cc_=V32FPkk%2Fhw%3D%3D; tg=0; mt=ci=11_1&np=;' \
         ' cookie2=33eddb22be1d3d5b77b03e97859175fb; v=0; _tb_token_=56b33ee3e0d5e; pnm_cku822=114%23jRCHPhCqTTobmob' \
         'rcW8C8%2F%2F0cO0zd%2FFcqJqsdjhGuvY8QUOHMc9zmu43EVTWRxvnmamvMkffzwTmC0UY8Wb6wbbFoJ2a3OmEvsbkT8TUo6gwTTsTnju' \
         '8fMoTDITaV4YjMVzpnSb7o6zmbRTTTj6Ufno9IjTsqpkwfoWnq6b82uWbJxNNPL1iq3X5NTZe%2F1IH3x7ihFzT18Aox6pgSX0LfAXjhYf' \
         'Ac8gYNR54EmcuB%2FR94f6k0YegViQE7b0JORk79NsS69eqeFrl1ZUz00%2F25j0gTlnEnpGOsqZmaR2wZegO9BIKcCmbib6dsjGQhtHKt' \
         '%2FW6mYcE%2BYgcuoqr4so6VcbIsslHcMn%2FvDGIJIVAJzAFYLQlo71NdghXCrwr; isg=BHNzJWsSKxOgt-DNmc5tFMHmAnddACxSpEE' \
         'vuyUQzRLJJJPGrXkJukW22xRv-F9i; x=e%3D1%26p%3D*%26s%3D0%26c%3D0%26f%3D0%26g%3D0%26t%3D0%26__ll%3D-1%26_ato%' \
         '3D0; swfstore=138867'
header = {                                                            #这个是要访问的网址的头
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET "
                     "CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko",
        "Aceept": 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        "cookie": cookie
    }
# url = "https://yumomolk.taobao.com/search.htm?spm=a1z10.1-c-s.w5002-14519245652.1.3d1a2979oORD61&search=y"
# def get_struct(url):
#     req = requests.get(url, headers=header)
#     req_text = BeautifulSoup(req.text,'lxml')
#     # print(req_text)
#     Wid = req_text.find('div',{'id': 'bd'}).find('div',{'class','J_TModule'})
#     # print(Wid)
#     wid = Wid['data-widgetid']
#     print(wid)                  #print检测一下获得的wid是否正确
#     # res_con = req.content  ###这个就是network里的XHR里指定的那个文件的response，是byte类型的
#     # _ksTS = res_text.find('a',{'href':'//yumomolk.taobao.com/search.htm'})['_ksTS']
#     # # respj = resp.json()
#     # # print(respj)
#     # resp_str = str(resp, encoding='utf-8',errors='ignore')          #json只能转化str类型，但是resp是byte类型的，# 所以要先转换为str类型，但是这样转化会报错
#     # # respj = json.loads(resp_str)                      #其实只要把我获取到的resp转化为字典形式就好
#     # print(resp_str)
#
#     # _ksTs=resp.attrs['_ksTs']
#     # print(_ksTs)
#     base_url = 'https://yumomolk.taobao.com/i/asynSearch.htm?_'
#     """下面这个这个网页的是请求参数表，可以用这个来写出那个request URL，
#     其中_ksTS是时间戳，wid是店铺id，path是路径，
#     spm是用来跟踪页面模块位置的编码"""
#     params = {
#                  '_ksTS': '1544335341760_264',
#                  'callback': 'jsonp265',
#                  'mid': 'w-14519245696-0',
#                  'wid': '14519245696',
#                  'path': '/category.htm',
#                  'search': 'y',
#                  'spm': 'a1z10.1-c-s.w5002-14519245652.1.3d1a2979oORD61'
#     }
#
#     struct = base_url+urlencode(params)
#     print(struct)#拼接出数据藏匿的request URL
#     return  struct
def get_page(url,P1):
        print("这是第"+P1+"页")
        time.sleep(2)
        # print(url)
        req = requests.get(url, headers=header)
        bsObj = BeautifulSoup(req.text, "lxml")
        # res = Soj_session.post(url + '/action.php?act=Login', data=data, headers=header)
        # # print(url)
        '''这条语句有时候能解决HTTP error 302
        (原理：默认情况下，当你进行网络请求后，响应体会立即被下载。
        你可以通过 stream 参数覆盖这个行为，推迟下载响应体)'''
        # r = requests.head(url,headers = headers,stream=True)
        # html = requests.get(url,headers=header)
        # print(r.headers['location'])
        # print(response)
        # bsObj = BeautifulSoup(req.text,'lxml')
        print(req.status_code)
        # print(bsObj.find("ul"))
        # print(bsObj)
        sitelist = []                       #定义商品链接列表
        img_list = []                       #定义图片链接列表
        title_list = []                     #定义标题列表
        "用正则来匹配所需要的内容，例如这里的"
        for link in bsObj.findAll("a",href=re.compile("\/item\.htm\?id=\d\d\d\d\d\d\d\d\d\d\d\d")):
            print(link)
            title = link.text
            # print(title)
            title1 = re.sub("\d","",string=title).strip(" ")         #提取出标题,注意按这种方式提取出来的有72个标题，其中只有24个是真标题
            # print(title1)
            title_list.append(title1)
            while '' in title_list:  # 这里用while语句将假标题移除
                title_list.remove('')
            # print(len(title_list))  # 这句话看一下标题的个数，是否将假标题都移除了
            img = link.find("img")
            print(img)
            try:                                        #这边用try，except是因为会报错，Nonetype错误，因为有些满足上述正则条件的标签里没有img这个节点，直接跳过错误就好
                # print(img["src"])
                img_url = "https:"+img["src"].strip(r"\"")     #图片的URL
                img_list.append(img_url)
                # urllib.request.urlretrieve(img_url, "%s.jpg" %title)   #下载图片
                # print(img_url)
            except TypeError as e:
                pass
            while '' in img_list:
                img_list.remove('')
            print(img_list)
            # print(img_list)
            # if 'src' in img.attrs:
            #     src = img.attrs["src"].strip("\\\"")
            #     print(src)
            # src = link.find("href")
            # print(src)
            # image = link.find("img")
            # print(image["src"])
            """我的理解：attrs把link都变成了字典形式，若字典里含有href，
                就把href所对应的值左右都去掉\",然后把值赋给link1"""
            if 'href' in link.attrs:

                    # print(link)
                    link1 = link.attrs['href'].strip("\\\"")
                    # print(link1)
                    sitelist.append(link1)                  #将获取到的link1组成一个列表

        """获取商品的原价，折后价，销量"""
        I = 0
        # for price_salenum in bsObj.findAll("div",class_=r"\"attribute\""):
        #     # print(price_salenum)
        #     try:
        #         s_prince = price_salenum.find(class_=r"\"s-price\"").text     #这个是获取到的原价,要用try，except的原因还是第一个商品没有s-price
        #         print(title_list[I]+'\n'+img_url+"\n原价："+s_prince+"元")
        #     except AttributeError as E:
        #         print(title_list[I]+"\n"+img_url+"\n原价："+"此商品无原价")
        #     c_price = price_salenum.find(class_=r"\"c-price\"").text    #这个是获取到的折后价格
        #
        #     print("折后价："+c_price+"元")
        #     sale_num  = price_salenum.find(class_=r"\"sale-num\"").text    #这个是获取到的销售件数
        #     print("已售："+sale_num+"件")
        #     I = I+1
        # print(len(sitelist))
        # sorted(sitelist)
        # print(sitelist[71])
        i=2
        while i<50:                     #把获取到的评论链接删除
            # print(i)
            # print(sitelist[i])
            sitelist.remove(sitelist[i])
            i=i+2
        # print(sitelist)
        # print(len(sitelist))
        sitelist2 = list(set(sitelist))         #去掉列表内重复元素
        # print(sitelist2)
        # print(len(sitelist2))
        for j in range(0,24):
            sitelist3 = 'https:'+sitelist2[j]
            # print(sitelist3)

        # for i in range(2,72,3):
        #     print(i)
        #     del sitelist[i]

        # urllib.request.urlretrieve()
        # result = bsObj.findAll("a",{"href":re.compile("\/item\.htm\?id=+")})
        # print(result)
        # result = bsObj.findAll(bsObj.a["href"])
        # get_site = re.match('^//item.taobao.com/item.htm?id=\d\d\d\d\d\d\d\d\d\d\d\d',bsObj)
        # print(bsObj)
        # print(get_site)
        # print(result)
        # result1 = result.find_all("class"=="productPrice")
        # result2 = re.match('^</span>',result1)
        # print(result.div.div.div.a)
        # print(result1)
        # print(result2)
        '''获取下一页,这里获取到的是店铺的下一页，不是那个接口那边的
        哈哈哈，事实证明这样写的话要解密requestURL，目前我还没写出来，
        就先不这样写了，取而代之，在原有的URL加上pageNo=i就ok了'''
        # for next_page in bsObj.findAll("a",class_=r"\"J_SearchAsync",text='下一页'):
        #         next_page1="https:"+next_page["href"].strip(r"\"")
        #         print(next_page1)
        # with open(r"C:\Users\asus\Desktop\try.doc",encoding='utf-8') as file:
        #     file.write("{},{},{},{},{}".format())
if __name__=='__main__':
    """第一步，先确定爬的网址，看所需的东西到底在哪，这边是在另外一个网页上，不在源代码
    具体操作：现在网上很多淘宝店铺都是用ajax，你想找到商品在哪的话，
    谷歌浏览器，右键空白处，检查，network，all，然后选择response，
    然后一个一个点击name那里，在右边的窗口能找到你所需要的东西，找到了的话，点headers，把里面的 Request URL复制下来，
    打开这个网页，东西在这里找就好了。这里下面的URL就是真正需要的URL"""
    # url=get_struct("https://yumomolk.taobao.com/search.htm?spm=a1z10.1-c-s.w5002-14519245652.1.3d1a2979oORD61&search=y")
    for P in range(1,38):
        P1 = str(P)
        url2= get_page("https://yumomolk.taobao.com/i/asynSearch.htm?_"
                       "ksTS=1544335341760_264&callback=jsonp265&mid=w-14519245696-0&"
                       "wid=14519245696&"
                       "path=/category.htm&"
                       "spm=a1z10.3-c-s.w4010-14519245694.2.2d01d514szJfVa&"
                       "search=y&pageNo"+P1,P1)               #将get_struct()里的拼接出来的struct传入get_page().
